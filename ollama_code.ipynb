{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pemLY9C-iIy"
      },
      "outputs": [],
      "source": [
        "# a notebook for working with LLMs locally using ollama models and langchain for prompt construction\n",
        "# this code analyzes data from r/antiwork to explore collective/individual action frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oRXphCG2_Ep"
      },
      "outputs": [],
      "source": [
        "# notebook setup, sometimes required\n",
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1wJscN551xw"
      },
      "outputs": [],
      "source": [
        "# install required packages\n",
        "!pip install langchain-ollama\n",
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H_F6t0C5PdE"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "import pandas as pd\n",
        "import ollama\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS1Sr1AV5TDW"
      },
      "outputs": [],
      "source": [
        "# a function to start the ollama server for running LLMs locally\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen(['ollama', 'serve'])\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "\n",
        "thread.start()\n",
        "time.sleep(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh1WNqc45lrQ"
      },
      "outputs": [],
      "source": [
        "# download the models (full list of options here: https://ollama.com/library)\n",
        "ollama.pull(\"llama3.2\")\n",
        "ollama.pull(\"mistral\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff6oeIBk5tPg"
      },
      "outputs": [],
      "source": [
        "# for importing data from google drive; otherwise, ignore this block\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "gdrive_path = \"/content/drive\" + \"/My Drive\" + \"/folder_name\"\n",
        "df = pd.read_csv(gdrive_path + 'data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7InBIAry950B"
      },
      "outputs": [],
      "source": [
        "# set the model (note: must be downloaded first in ollama.pull step)\n",
        "model = OllamaLLM(model=\"llama3.2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY11EB25RjKB"
      },
      "outputs": [],
      "source": [
        "# set the system message, which assigns the model an \"identity\"\n",
        "system_message = SystemMessage(\n",
        "    content= \"You are Doreen, and you classify posts from the subreddit Antiwork as if you are the moderator of the community. Antiwork users discuss issues related to capitalism and work.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zQ44zcSXABM"
      },
      "outputs": [],
      "source": [
        "# function to query the LLM, including an input prompt and text data in \"messages\" format, and save the response\n",
        "def get_model_response(text):\n",
        "    prompt = f\"\"\"\n",
        "    Doreen, please state whether this post contains an individual call to action,\n",
        "    a collective call to action, both an individual and a collective call to action,\n",
        "    or no call to action. Explain your reasoning and what influenced your decision: {text}\n",
        "    \"\"\"\n",
        "\n",
        "  messages = [\n",
        "      system_message,\n",
        "      HumanMessage(content=prompt)\n",
        "  ]\n",
        "\n",
        "  response = model.invoke(messages)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-IGGvNd96fQ"
      },
      "outputs": [],
      "source": [
        "# create a new column in the dataframe to save the output from llama based on text input from the 'selftext' column in the dataframe, which contains full texts of r/Antiwork posts\n",
        "df['llama_response'] = df['selftext'].apply(get_model_response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first 5 rows of the new column\n",
        "df['llama_response'].head()"
      ],
      "metadata": {
        "id": "k3Pbl3m97bDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeQ-xctnNX2D"
      },
      "outputs": [],
      "source": [
        "# repeat the analysis with mistral, starting with setting the model parameter\n",
        "model = OllamaLLM(model=\"mistral\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XveewJcJNbBm"
      },
      "outputs": [],
      "source": [
        "# create a new column for the output from mistral based on the text column in the dataframe\n",
        "df['mistral_response'] = df['selftext'].apply(get_model_response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first 5 rows of the full dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "s_dkTK9Fw3tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vdf6lPVpb3Vv"
      },
      "outputs": [],
      "source": [
        "# save the dataframe (this example saves to the mounted google drive location)\n",
        "df.to_csv(gdrive_path + '/llm_results.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}